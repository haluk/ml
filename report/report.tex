
\documentclass[12pt]{article}
\usepackage{mathrsfs}

\usepackage{setspace}
\doublespacing

\usepackage[a4paper, margin=1in]{geometry}
\usepackage{float}
\usepackage[shortlabels]{enumitem}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}




\title{Homework 1 ID3 Implementation}
\author{
        Zeynep Mutlu Hakguder \\
                Department of Electrical and Computer Engineering\\
        University of Nebraska-Lincoln, USA\\
        zeynep.hakguder@huskers.unl.edu
            \and
        Haluk Dogan\\
        Department of Electrical and Computer Engineering\\
        University of Nebraska-Lincoln, USA\\
        haluk.dogan@huskers.unl.edu
}
\date{\today}


\begin{document}
\maketitle

\newpage
\tableofcontents

\newpage
\section{Question 1}
The training data consists of $6$ examples each with a single
integer-valued attribute and a binary label. The target function $\mathcal{C}$ is
represented as a single interval using two points \textit{a} and \textit{b}.

\begin{table}[H]
\centering
\begin{tabular}{|l|l||l|l||l|l|}
\hline
1 & - & 6  &   & 11 &   \\ \hline
2 &   & 7  & + & 12 & - \\ \hline
3 &   & 8  & + & 13 &   \\ \hline
4 &   & 9  &   & 14 &   \\ \hline
5 & + & 10 &   & 15 & - \\ \hline
\end{tabular}
\caption{Training Set}
\label{my-label}
\end{table}

\begin{enumerate}[(a)]
   \item \label{a} A hypothesis consistent with the training set would be
obtained if $a = 5$ and $b = 8$ so that an instance x is labeled positive
if and only if $5 \leq x \leq 8$.
   \item The version space is the subset of hypothesis space
$\mathcal{H}$ (which is chosen as the family of single intervals and
is guaranteed to include target concept) that is consistent with the
training set. For example, in the interval given in \ref{a}, all
positive training examples are contained, if it was any smaller, it
would exclude a positive training example and be inconsistent with the
data. We could expand the interval to $2 \leq x \leq 11$, this is the
largest interval that is consistent with the training set, if we were
to expand it any further we would end up including a negative example
in the interval that represents the target function. The version space
is the set of all such hypotheses (represented by intervals) that are
consistent with the training set. The interval's lower bound can be
any of $2, 3, 4$ or $5$, its upper bound can be any of $8, 9, 10$ or
$11$, the size of the version space is $4 \times 4 = 16$.
   \item To decrease the size of the version space we can specify the
query $q_1 = 10$. If the label for $10$ is:
      \begin{itemize}
         \item positive, the smallest interval consistent with our
training data and $q_1$ would be $5 \leq x \leq 10$. We would
eliminate $8$ hypotheses that assign negative labels to $9$ and $10$
from the version space.
         \item negative, the smallest interval consistent with our
training data and $q_1$ would be $5 \leq x \leq 9$. We would eliminate
$8$ hypotheses that assign positive labels to $10$ and $11$ from the
version space.
      \end{itemize}           
If we set $q_2 = 6$, this query wouldn't change the size of the
version space regardless of the answer.
   \item If we have a training set with three examples $\left\{
\left(x_{1}, - \right), \left(x_{2}, + \right), \left(x_{3}, -
\right)\right\}$, where $0 < x_1 < x_2 < x_3$, in order to reduce the
size of the version space, we can form queries in a manner similar to
binary search. First, to find the upper bound of the interval we ask
the label of the midpoint between $x_2$ and $x_3$, depending on the
label we receive we form our next query; i.e if it is negatively
labeled we ask the label for the midpoint between the last query and
$x_2$, if it is positively labeled we ask the label for the midpoint
between the last query and $x_3$. Similarly, to obtain the lower
boundary we ask the label for the midpoint between $x_1$ and $x_2$, if
it is negatively labeled we ask the label for the midpoint between the
last query and $x_2$, if it's positively labeled we ask the label for
the midpoint between the last query and $x_1$. Below is an example
showing one of the worst case scenarios with training set $\left\{
\left(1, - \right), \left(9, + \right), \left(17, - \right)\right\}$.
\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{img/hw1_1d}
  \caption{Query example to decrease version space size.}
\end{figure}
\end{enumerate}

\section{Question 2}
\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{img/hw1_2a}
  \caption{$A \wedge \left[ \thicksim B \right]$}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{img/hw1_2b}
  \caption{$A\vee\left[ B\wedge C \right]$}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{img/hw1_2c}
  \caption{$A\oplus B$}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{img/hw1_2d}
  \caption{$\left[  A \wedge B\right] \vee \left[  C\wedge D \right]$}
\end{figure}


\section{Question 3}

\begin{enumerate}[(a)]
  \item The entropy of the dataset is calculated as follows:
    \begin{center}
      \begin{align*}
        I&=-{ p }^{ + }\log _{ 2 }{ { p }^{ + } } -{ p }^{ - }\log _{ 2 }{ { p }^{ - } } \\
         &=-\frac { 3 }{ 6 } \log _{ 2 }{ \frac { 3 }{ 6 }  } -\frac { 3 }{ 6 } \log _{ 2 }{ \frac { 3 }{ 6 }  } \\
         &=1
      \end{align*}
    \end{center}
  \item The ID3 algorithm would choose the attribute with smallest impurity which is calculated as below:
    \begin{center}
      \begin{align*}
        { I }^{ \prime  }\left( { a }_{ i } \right) &=\frac { \left| T \right|  }{ \left| T+F \right|  } \left( -{ p }_{ T }^{ + }\log _{ 2 }{ { p }_{ T }^{ + } } -{ p }_{ T }^{ - }\log _{ 2 }{ { p }_{ T }^{ - } }  \right) \\
        &+ \frac { \left| F \right|  }{ \left| T+F \right|  } \left( -{ p }_{ F }^{ + }\log _{ 2 }{ { p }_{ F }^{ + }-{ p }_{ F }^{ - } } \log _{ 2 }{ { p }_{ F }^{ - } }  \right) 
      \end{align*}
    \end{center}
    where $i=1,2$.
    \begin{center}
      \begin{align*}
        { I }^{ \prime  }({ a }_{ 1 })&=\frac { 3 }{ 6 } \left( -\frac { 2 }{ 3 } \log _{ 2 }{ \frac { 2 }{ 3 }  } -\frac { 1 }{ 3 } \log _{ 2 }{ \frac { 1 }{ 3 }  }  \right) +\frac { 3 }{ 6 } \left( -\frac { 1 }{ 3 } \log _{ 2 }{ \frac { 1 }{ 3 } -\frac { 2 }{ 3 } \log _{ 2 }{ \frac { 2 }{ 3 }  }  }  \right) =0.9183 \\
        { I }^{ \prime  }({ a }_{ 2 })&=\frac { 4 }{ 6 } \left( -\frac { 2 }{ 4 } \log _{ 2 }{ \frac { 2 }{ 4 }  } -\frac { 2 }{ 4 } \log _{ 2 }{ \frac { 2 }{ 4 }  }  \right) +\frac { 2 }{ 6 } \left( -\frac { 1 }{ 2 } \log _{ 2 }{ \frac { 1 }{ 2 } -\frac { 1 }{ 2 } \log _{ 2 }{ \frac { 1 }{ 2 }  }  }  \right) =1.0
      \end{align*}
    \end{center}
Since ${ I }^{ \prime  }({ a }_{ 1 })<{ I }^{ \prime  }({ a }_{ 2 })$, \textit{ID3} algorithm would choose $a_1$ next.
\end{enumerate}
\section{Question 4}
TODO
\subsection{Introduction}
This is time for all good men to come to the aid of their party!

\paragraph{Outline}
lorem

\subsection{Data}

\subsection{Implementation}
In this section we describe the results.

\subsection{Results}
We worked hard, and achieved very little.

\subsection{Discussion}
lorem

\bibliographystyle{abbrv}
\bibliography{report}

\section{Appendix}

\end{document}

